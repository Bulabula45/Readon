{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from Bio import SeqIO, AlignIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Files and softwares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "rm -rf ./downloads\n",
    "mkdir -p ./downloads/bin\n",
    "\n",
    "wget -p https://ftp.ensembl.org/pub/release-110/fasta/homo_sapiens/cds/Homo_sapiens.GRCh38.cds.all.fa.gz -O ./downloads/Homo_sapiens.GRCh38.cds.all.fa.gz\n",
    "gunzip ./downloads/Homo_sapiens.GRCh38.cds.all.fa.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./downloads/Homo_sapiens.GRCh38.cds.all.tsv\", 'w') as w:\n",
    "    with open(\"./downloads/Homo_sapiens.GRCh38.cds.all.fa\") as f:\n",
    "        for q in f:\n",
    "            if q[0] == '>':\n",
    "                print(q.rstrip()[1:].split('.')[0], end='\\t', file=w)\n",
    "            else:\n",
    "                print(q.rstrip(), end='\\n', file=w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_string(lst):\n",
    "    d = {}\n",
    "    for i in lst:\n",
    "        if i in d:\n",
    "            d[i] += 1\n",
    "        else:\n",
    "            d[i] = 1\n",
    "    return max(d, key=d.get)\n",
    "\n",
    "def get_position(enst):\n",
    "    obj = subprocess.Popen(\n",
    "    \"grep $'\"+enst+\"'\\t ./downloads/Ensembl_Homo_sapiens.GRCh38.110.bed\",\n",
    "    shell=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    "    )\n",
    "    enst_pos = obj.stdout.read() \n",
    "    return enst_pos.decode('utf-8').strip()\n",
    "\n",
    "def get_seq(enst):\n",
    "    obj = subprocess.Popen(\n",
    "    'grep '+enst+' ./downloads/Homo_sapiens.GRCh38.cds.all.tsv',\n",
    "    shell=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    "    )\n",
    "    seq = obj.stdout.read() \n",
    "    return seq.decode('utf-8').strip()\n",
    "\n",
    "def to_onehot(seq):\n",
    "    atgc_dict = {'A':0, 'T':1, 'G':2, 'C':3}\n",
    "    M = np.zeros((max_len, 4))\n",
    "    for i, s in enumerate(seq):\n",
    "        M[i, atgc_dict[s.upper()]] = 1\n",
    "    return M\n",
    "\n",
    "def onehot_to_seq(M):\n",
    "    atgc_dict = {0:'A', 1:'T', 2:'G', 3:'C'}\n",
    "    seq = \"\"\n",
    "    for i in range(M.shape[0]):\n",
    "        seq += atgc_dict[np.argmax(M[i,:])]\n",
    "    return seq\n",
    "\n",
    "\n",
    "def aln_clustal():\n",
    "    obj = subprocess.Popen(\n",
    "    'clustalo -i ./downloads/tmp_for_aligning.fa > ./downloads/alignment.fa',\n",
    "    shell=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    "    ).wait()\n",
    "\n",
    "    alignment = AlignIO.read(\"alignment.fa\", \"fasta\")\n",
    "\n",
    "def getConsensus(alignment):\n",
    "    consensus = \"\"\n",
    "    for i in range(len(alignment[0])):\n",
    "        base_counts = {\"A\": 0, \"C\": 0, \"G\": 0, \"T\": 0, \"-\": 0}\n",
    "        for record in alignment:\n",
    "            if i < len(record):\n",
    "                base = record[i]\n",
    "                base_counts[base] += 1\n",
    "            else:\n",
    "                base_counts['-'] += 1\n",
    "        if max(base_counts, key=base_counts.get) == '-':\n",
    "            del base_counts['-']\n",
    "            consensus += max(base_counts, key=base_counts.get)    \n",
    "        consensus += max(base_counts, key=base_counts.get)\n",
    "    \n",
    "    return consensus\n",
    "\n",
    "\n",
    "def multiAlign(sequenced_seqs, ref_seq=None, ref_seq_top5=None):\n",
    "    new_sequenced_seqs, flags = [], []\n",
    "    if ref_seq == None:\n",
    "        ref_seq = sequenced_seqs[0]\n",
    "    int_flag = np.zeros((len(sequenced_seqs), len(ref_seq)))\n",
    "\n",
    "    for idx,seq in enumerate(sequenced_seqs):\n",
    "        if ref_seq_top5 != None:\n",
    "            start = seq.find(ref_seq_top5)\n",
    "        else:\n",
    "            start = 0\n",
    "        seq = seq[start:]\n",
    "        new_seq = \"\"\n",
    "        new_flag = \"\"\n",
    "        len_seq = len(seq)\n",
    "        len_ref_seq = len(ref_seq)\n",
    "        i, j = 0, 0\n",
    "        while(i<len_seq and j<len_ref_seq):\n",
    "            if ref_seq[j] == seq[i]:\n",
    "                new_seq += seq[i]\n",
    "                new_flag += \"=\"\n",
    "\n",
    "            elif (i+1<len_seq and j+2<len_ref_seq) and seq[i] == ref_seq[j+1] and seq[i+1] == ref_seq[j+2]:\n",
    "                new_seq += '-'\n",
    "                new_seq += seq[i]\n",
    "                new_seq += seq[i+1]\n",
    "                new_flag += \"===\"\n",
    "                i += 1\n",
    "                j += 2\n",
    "            elif (i+1<len_seq and j+3<len_ref_seq) and seq[i] == ref_seq[j+2] and seq[i+1] == ref_seq[j+3]:\n",
    "                new_seq += '--'\n",
    "                new_seq += seq[i]\n",
    "                new_seq += seq[i+1]\n",
    "                new_flag += \"====\"\n",
    "                i += 1\n",
    "                j += 3\n",
    "            elif (i+1<len_seq and j+4<len_ref_seq) and  seq[i] == ref_seq[j+3] and seq[i+1] == ref_seq[j+4]:\n",
    "                new_seq += '---'\n",
    "                new_seq += seq[i]\n",
    "                new_seq += seq[i+1]\n",
    "                new_flag += \"=====\"\n",
    "                i += 1\n",
    "                j += 4\n",
    "            else:\n",
    "                new_seq += seq[i]\n",
    "                new_flag += \"X\"\n",
    "                int_flag[idx, j] = 1\n",
    "                \n",
    "            i += 1\n",
    "            j += 1\n",
    "            \n",
    "        new_sequenced_seqs.append(new_seq)\n",
    "        flags.append(new_flag)\n",
    "    return new_sequenced_seqs, flags, int_flag\n",
    "\n",
    "def getEnst(geneName1):\n",
    "    command = f\"\"\"grep -E 'transcript.*gene_name \"{geneName1}\"' ./downloads/Homo_sapiens.GRCh38.110.gtf | awk '$3 == \"exon\" {{split($0, a, \"; \"); for (i in a) if (match(a[i], /^exon_number \"[0-9]+\"/)) {{split(a[i], b, \" \"); if (b[2] > max) {{max=b[2]; line=$0}}}}}} END {{print line}}'\n",
    "    \"\"\"\n",
    "    print(command)\n",
    "    proc = subprocess.Popen(\n",
    "        command, \n",
    "        stdin=None, \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE, \n",
    "        shell=True)\n",
    "    outinfo, errinfo = proc.communicate()\n",
    "    return outinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"./downloads/example.reads.fa\"\n",
    "ratio_cutoff = 0.5\n",
    "out_file = \"./downloads/example.reads.fates.tsv\"\n",
    "\n",
    "name_to_seq = {}\n",
    "name_to_enst = {}\n",
    "id_1, id_2, enst_1, name_1, enst_2, name_2, concat_name, concat_name_r = \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "full_id_list = []\n",
    "NMD_or_PROTEIN = {}\n",
    "idx = 0\n",
    "\n",
    "\n",
    "ENST_MODE = True\n",
    "'''\n",
    "ENST_MODE,\n",
    ">ENST00000541412,TMEM14C|ENST00000481240,TMEM14B|6|1|5fd55169-79d9-4755-b60d-ebe7d51bec93\n",
    "TACGCTGATATTGCTGGGGAGAGACTGGCTGCTGTG...\n",
    "otherwise,\n",
    ">Novel3_177_perfect_19584_F_0_2803_0,ZBED1,DHRSX\n",
    "ACCTCCAACCTGTCCTACCACCTGGAGAAGAACCACCCCGAGGAATTCTGCGAGTTCGTCAAGAGCAACACGGAGCAGATGCGTGAAGCCTTCGCCACCGCCTTCTCCAAGCTGAAGCCCGAGTCGTCCCAGCAGCCCGGGCAGGACGCGCTGGCCGTCAAGGCCGGCCACGGCTACGACAGCAAGAAGCAGCAGGAGCTGACGGCCGCCGTGCTGGGCCTCATCTGCGAGGGGCTGTACCCAGCCTCCATCGTGGACGAGCCCACCTTCAAGGTGCTGCTGAAGACGGCCGACCCCCGGTATGAGCTGCCCAGCCGGAAGTACATCTCTACCAAGGCCATCCCTGAGAAGTACGGGGCCGTCCGGGAGGTGATCCTGAAGGAGCTGGCCGAGGCCACCTGGTGTGGCATCTCCACCGACATGTGGAGGAGTGAGAATCAGAACCGCGCCTACGTCACGCTGGCCGCCCACTTCCTGGGCCTGGGCGCCCCCAACTGCCTGTCCATGGGCTCCCGCTGCCTGAAGACCTTCGAGGTGCCCGAAGAGAACACGGCGGAGACCATCACGCGAGTGCTCTATGAGGTCTTCATCGAGTGGGGCATCAGCGCCAAGGTCTTCGGGGCCACCACCAACTATGGCAAGGACATCGTGAAGGCGTGCTCCCTGCTGGACGTCGCAGTGCACATGCCCTGCCTGGGCCACACCTTCAATGCCGGCATCCAGCAGGCCTTCCAGCTCCCGAAGCTGGGGGCGCTGCTGTCGCGCTGCCGCAAACTGGTGGAGTACTTCCAGCAGTCTGCCGTGGCCATGTACATGCTCTATGAGAAGCAGAAGCAGCAGAACGTGGCCCACTGCATGCTGGTGAGCAACCGCGTCTCCTGGTGGGGGAGCACGCTGGCCATGCTGCAGCGCCTCAAGGAGCAGCAGTTCGTCATCGCCGGGGTCTTGGTGGAGGACAGCAACAACCACCACCTCATGCTGGAGGCCAGCGAGTGGGCCACCATCGAGGGGCTGGTGGAGCTCCTGCAGCCCTTCAAGCAGGTGGCCGAGATGCTGTCGGCCTCCAGGTACCCCACCATCAGCATGGTGAAGCCGCTGCTGCACATGCTCCTGAACACCACGCTCAACATCAAGGAGACCGACTCCAAGGAGCTCAGCATGGCCAAGGAGGTCATCGCCAAGGAGCTTTCCAAGACCTACCAGGAGACGCCCGAGATCGACATGTTTCTCAACGTGGCCACCTTCCTGGACCCCCGCTACAAGAGGCTGCCCTTCCTCTCCGCCTTCGAGCGGCAGCAGGTGGAGAATCGCGTGGTGGAAGAGGCCAAGGGCCTGCTGGACAAGGTCAAAGACGGCGGCTACCGGCCGGCTGAGGACAAGATCTTCCCGGTGCCCGAGGAGCCTCCCGTCAAGAAGCTCATGCGGACATCCACGCCGCCGCCCGCCAGCGTCATCAACAACATGCTGGCCGAGATCTTCTGCCAGACAGGCGGCGTGGAGGACCAGGAAGAGTGGCATGCCCAGGTGGTGGAGGAGCTGAGCAACTTCAAGTCCCAGAAGGTGCTTGGCCTCAACGAAGACCCCCTCAAGTGGTGGTCAGACCGCCTGGCCCTCTTCCCCCTGCTGCCCAAGGTGCTGCAGAAGTACTGGTGCGTGACGGCCACGCGCGTCGCCCCTGAGCGTCTCTTCGGATCCGCCGCCAACGTGGTCAGCGCCAAGAGGAACCGGCTGGCTCCCGCGCACGTGGACGAGCAGGTGTTTCTGTATGAGAACGCCCGGAGTGGGGCAGAGGCGGAACCCGAGGACCAGGACGAGGGGGAGTGGGGCCTGGACCAGGAGCAGGTGTTCTCCTTGGGGGATGGCGTCAGCGGCGGTTTCTTTGGCATTAGGGACAGCAGCTTCCTGTAGATGTCGCCATTGTCTGCGGCGCGGGCGGCCCTGCGGGTCTACGCGGTAGGCGCCGCGGTGATCCTGGCGCAGCTGCTGCGGCGCTGCCGCGGGGGCTTCCTGGAGCCAGTTTTCCCCCCACGACCTGACCGTGTCGCTATAGTGACGGGAGGGACAGATGGCATTGGCTATTCTACAGCGAAGCATCTGGCGAGACTTGGCATGCATGTTATCATAGCTGGAAATAATGACAGCAAAGCCAAACAAGTTGTAAGCAAAATAAAAGAAGAAACCTTGAACGACAAAGTGGAATTTTTATACTGTGACTTGGCTTCCATGACTTCCATCCGGCAGTTTGTGCAGAAGTTCAAGATGAAGAAGATTCCTCTCCATGTCCTGATCAACAATGCTGGGGTGATGATGGTCCCTCAGAGGAAAACCAGAGATGGATTCGAAGAACATTTCGGCCTGAACTACCTAGGGCACTTCCTGCTGACCAACCTTCTCTTGGATACGCTGAAAGAGTCTGGGTCCCCTGGCCACAGTGCGAGGGTGGTCACCGTCTCCTCTGCCACCCATTACGTCGCTGAGCTGAACATGGATGACCTTCAGAGCAGTGCCTGCTACTCACCCCACGCAGCCTACGCCCAGAGCAAGCTGGCCCTTGTCCTGTTCACCTACCACCTCCAGCGGCTGCTGGCGGCTGAGGGAAGCCACGTGACCGCCAACGTGGTGGACCCCGGGGTGGTCAACACGGACGTCTACAAGCACGTGTTCTGGGCCACCCGTCTGGCGAAGAAGCTTCTCGGCTGGTTGCTTTTCAAGACCCCCGATGAAGGAGCGTGGACTTCCATCTACGCAGCAGTCACCCCAGAGCTGGAAGGAGTTGGTGGCCATTACCTATACAACGAGAAAG\n",
    "'''\n",
    "with open(in_file) as f:\n",
    "    for q in f:\n",
    "        if '>' in q:\n",
    "            full_id_list.append(q.rstrip()[1:])\n",
    "            if ENST_MODE:\n",
    "                id_1 = q.rstrip()[1:].split(\"|\")[0]\n",
    "                id_2 = q.rstrip()[1:].split(\"|\")[1]\n",
    "                enst_1, name_1 = id_1.split(',')[0].split('.')[0], id_1.split(',')[1]\n",
    "                enst_2, name_2 = id_2.split(',')[0].split('.')[0], id_2.split(',')[1]\n",
    "                concat_name = name_1 + '_' + name_2\n",
    "                concat_name_r = name_1 + '_' + name_2\n",
    "                if concat_name in name_to_enst.keys():\n",
    "                    name_to_enst[concat_name].append((enst_1, enst_2))\n",
    "                elif concat_name_r in name_to_enst.keys():\n",
    "                    name_to_enst[concat_name_r].append((enst_2, enst_1))\n",
    "                else:\n",
    "                    name_to_enst[concat_name] = [(enst_1, enst_2)]\n",
    "\n",
    "            else:\n",
    "                name_1 = q.rstrip().split(',')[1]\n",
    "                name_2 = q.rstrip().split(',')[2]\n",
    "                concat_name = name_1 + '_' + name_2\n",
    "                concat_name_r = name_1 + '_' + name_2\n",
    "                if concat_name not in name_to_enst.keys() and concat_name_r not in name_to_enst.keys():\n",
    "                    outinfo = getEnst(name_1).decode('gbk')\n",
    "                    matches = re.findall(r'transcript_id \"(.*?)\"', outinfo)\n",
    "                    enst1 = matches[0]\n",
    "                    outinfo = getEnst(name_2).decode('gbk')\n",
    "                    matches = re.findall(r'transcript_id \"(.*?)\"', outinfo)\n",
    "                    enst2 = matches[0]\n",
    "                    name_to_enst[concat_name] = [(enst_1, enst_2)]\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            seq = q.rstrip()\n",
    "            if concat_name in name_to_seq.keys():\n",
    "                name_to_seq[concat_name].append(seq)\n",
    "            elif concat_name_r in name_to_seq.keys():\n",
    "                name_to_seq[concat_name_r].append(seq)\n",
    "            else:\n",
    "                name_to_seq[concat_name] = [seq]\n",
    "\n",
    "record_idx = 0\n",
    "\n",
    "\n",
    "for name, ensts_1_and_2 in name_to_enst.items():\n",
    "\n",
    "    ensts_1_and_2 = np.array(ensts_1_and_2)\n",
    "    sequenced_seqs = name_to_seq[name]\n",
    "    seqs_num = len(sequenced_seqs)\n",
    "    \n",
    "    enst1 = most_frequent_string(ensts_1_and_2[:,0])\n",
    "    enst2 = most_frequent_string(ensts_1_and_2[:,1])\n",
    "\n",
    "\n",
    "    enst1_pos = get_position(enst1).split('\\t')\n",
    "    enst2_pos = get_position(enst2).split('\\t')\n",
    "    enst1_start, enst1_end, strand = enst1_pos[1], enst1_pos[2], enst1_pos[5]\n",
    "    enst2_start, enst2_end = enst2_pos[1], enst2_pos[2]\n",
    "\n",
    "\n",
    "    if strand == '-1' or strand == '-':\n",
    "        if enst1_end > enst2_end:\n",
    "            enst_upstream = enst1 \n",
    "            enst_downstream = enst2\n",
    "        else:\n",
    "            enst_upstream = enst2 \n",
    "            enst_downstream = enst1\n",
    "    elif strand == '1' or strand == '+':\n",
    "        if enst1_start < enst2_start:\n",
    "            enst_upstream = enst1 \n",
    "            enst_downstream = enst2\n",
    "        else:\n",
    "            enst_upstream = enst2 \n",
    "            enst_downstream = enst1\n",
    "\n",
    "\n",
    "    if enst_upstream == enst1:\n",
    "        start = int(enst2_pos[1])\n",
    "        thick_start = int(enst2_pos[6])\n",
    "        sizes = [int(i) for i in enst2_pos[-2].split(',')[:-1]]\n",
    "        shift = [int(i) for i in enst2_pos[-1].split(',')[:-1]]\n",
    "        juncs = [start+i+j for i,j in zip(shift, sizes)]\n",
    "        thick_end = int(enst2_pos[7])\n",
    "        flag = True\n",
    "        for idx,i in enumerate(juncs):\n",
    "            if flag and i > thick_start:\n",
    "                rel = int(np.sum(sizes[:idx])) + thick_start - (start+shift[idx])\n",
    "                flag = False\n",
    "            if i > thick_end:\n",
    "                exon_exon_junc = np.sum(sizes[:idx]) - rel\n",
    "                break\n",
    "    else:\n",
    "        start = int(enst1_pos[1])\n",
    "        thick_start = int(enst1_pos[6])\n",
    "        sizes = [int(i) for i in enst1_pos[-2].split(',')[:-1]]\n",
    "        shift = [int(i) for i in enst1_pos[-1].split(',')[:-1]]\n",
    "        juncs = [start+i+j for i,j in zip(shift, sizes)]\n",
    "        thick_end = int(enst1_pos[7])\n",
    "        flag = True\n",
    "        for idx,i in enumerate(juncs):\n",
    "            if flag and i > thick_start:\n",
    "                rel = int(np.sum(sizes[:idx])) + thick_start - (start+shift[idx])\n",
    "                flag = False\n",
    "            if i > thick_end:\n",
    "                exon_exon_junc = int(np.sum(sizes[:idx])) - rel\n",
    "                break\n",
    "    \n",
    "\n",
    "    ref_seq = get_seq(enst_upstream).split('\\t')[1]\n",
    "    ref_seq_top5 = ref_seq[:5]\n",
    "    ref_seq_downstream = get_seq(enst_downstream).split('\\t')[1]\n",
    "    try:\n",
    "        ref_seq_downstream_top5 = ref_seq_downstream[exon_exon_junc:exon_exon_junc+5]\n",
    "    except:\n",
    "        print(exon_exon_junc)\n",
    "        pass\n",
    "\n",
    "    cut_sequenced_seqs = []\n",
    "    for query_seq in sequenced_seqs:\n",
    "        last_p = query_seq.find(ref_seq_downstream_top5)\n",
    "        if last_p == -1:\n",
    "            cut_sequenced_seqs.append(query_seq)\n",
    "        else:\n",
    "            cut_sequenced_seqs.append(query_seq[:last_p])\n",
    "\n",
    "\n",
    "    ref_len = len(ref_seq)\n",
    "    new_sequenced_seqs,flags, int_flag = multiAlign(cut_sequenced_seqs, ref_seq, ref_seq_top5)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        new_sequenced_seqs.append(ref_seq)\n",
    "        \n",
    "        sum_of_flag = np.sum(int_flag, axis=0)\n",
    "        inserted_exon_start = 0\n",
    "        for i in range(sum_of_flag.shape[0]):\n",
    "            if sum_of_flag[i] > int_flag.shape[0]*0.6 and sum_of_flag[i+1] > int_flag.shape[0]*0.6:\n",
    "                inserted_exon_start = i - i % 3\n",
    "                break\n",
    "        new_new_sequenced_seqs = []\n",
    "        for seq, new_seq in zip(cut_sequenced_seqs, new_sequenced_seqs):\n",
    "            actual_start = inserted_exon_start - new_seq[:inserted_exon_start].count('-')\n",
    "            new_new_sequenced_seqs.append(seq[actual_start:])\n",
    "        \n",
    "        # with open(\"tmp_for_aligning.fa\", \"w\") as f:\n",
    "        #     for i, seq in enumerate(new_new_sequenced_seqs):\n",
    "        #         print(\">\"+str(i)+\"\\n\"+seq, file=f)\n",
    "\n",
    "        \n",
    "        # using clustal to align, and get consensus\n",
    "        # consensus = aln_clustal()\n",
    "        STOP_CODONS = ['TAG','TAA','TGA']\n",
    "        truncted_seqs,_,_ = multiAlign(new_new_sequenced_seqs)\n",
    "        \n",
    "        corrected_seq = getConsensus(truncted_seqs)\n",
    "        # truncated_length = len(corrected_seq) * ratio_cutoff\n",
    "        truncated_length = 55\n",
    "        for i in range(0, len(corrected_seq), 3):\n",
    "            if corrected_seq[i:i+3] in STOP_CODONS:\n",
    "                if len(corrected_seq)-i > truncated_length:\n",
    "                    NMD_or_PROTEIN[record_idx] = (\"NMD\", corrected_seq, len(corrected_seq)-i)\n",
    "                else:\n",
    "                    NMD_or_PROTEIN[record_idx] = (\"PROTEIN\", corrected_seq, len(corrected_seq)-i)\n",
    "                # record_idx += 1\n",
    "                # break\n",
    "        record_idx += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "with open(out_file, 'w') as w:\n",
    "    for id, value in NMD_or_PROTEIN.items():\n",
    "        prediction,seq,_ = value\n",
    "        id1, id2= full_id_list[id].split('|')[0], full_id_list[id].split('|')[1]\n",
    "        id1, id2 = id1.split(',')[1], id2.split(',')[1]\n",
    "        print('>'+id1+','+id2+','+prediction+'\\n'+seq, file=w)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
